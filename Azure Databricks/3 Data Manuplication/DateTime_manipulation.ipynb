{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DateTime manipulation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO0AYJKMJnoRsS8xbE7Fey/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anil-chhetri/Miscellaneous/blob/main/Azure%20Databricks/3%20Data%20Manuplication/DateTime_manipulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bzrFTvvrbqS1"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = (SparkSession.builder\n",
        "        .master(\"local\")\n",
        "        .appName(\"Colab\")\n",
        "        .config('spark.ui.port', '4050')\n",
        "        .getOrCreate())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy = spark.createDataFrame([('x',)]).toDF('col')\n",
        "dummy.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnZya2e3dJ7c",
        "outputId": "85808060-7a80-48c8-962e-81e429d926f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "|col|\n",
            "+---+\n",
            "|  x|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as f"
      ],
      "metadata": {
        "id": "h0Cc87lUdgqv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(f.current_date)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5T_H49adTb3",
        "outputId": "3a9492ab-d7d5-4375-ca2e-39e06404090e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function current_date in module pyspark.sql.functions:\n",
            "\n",
            "current_date()\n",
            "    Returns the current date at the start of query evaluation as a :class:`DateType` column.\n",
            "    All calls of current_date within the same query return the same value.\n",
            "    \n",
            "    .. versionadded:: 1.5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy.select(f.current_date()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdQ3V16jdYSn",
        "outputId": "c7ace137-260f-4ce6-cdfb-cf253a40c6ce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+\n",
            "|current_date()|\n",
            "+--------------+\n",
            "|    2022-05-14|\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(f.current_timestamp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf5rRbhsdqmp",
        "outputId": "b347a867-2adc-40d2-eb74-8ae4e8292efb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function current_timestamp in module pyspark.sql.functions:\n",
            "\n",
            "current_timestamp()\n",
            "    Returns the current timestamp at the start of query evaluation as a :class:`TimestampType`\n",
            "    column. All calls of current_timestamp within the same query return the same value.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy.select(f.current_timestamp()).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh33r_2ddwMo",
        "outputId": "0a52b164-1abe-4cc6-a1c2-f9837d278be3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+\n",
            "|current_timestamp()    |\n",
            "+-----------------------+\n",
            "|2022-05-14 07:29:12.755|\n",
            "+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(f.date_format)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POitOtGOd4pZ",
        "outputId": "cb3b41ee-3cae-4c2b-e95a-47b3d5ad0278"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function date_format in module pyspark.sql.functions:\n",
            "\n",
            "date_format(date, format)\n",
            "    Converts a date/timestamp/string to a value of string in the format specified by the date\n",
            "    format given by the second argument.\n",
            "    \n",
            "    A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n",
            "    pattern letters of `datetime pattern`_. can be used.\n",
            "    \n",
            "    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
            "    \n",
            "    .. versionadded:: 1.5.0\n",
            "    \n",
            "    Notes\n",
            "    -----\n",
            "    Whenever possible, use specialized functions like `year`.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
            "    >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n",
            "    [Row(date='04/08/2015')]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(f.to_date)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUvLFslpeHx4",
        "outputId": "38a4a350-95f6-44c8-e849-e673d3a97c88"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function to_date in module pyspark.sql.functions:\n",
            "\n",
            "to_date(col, format=None)\n",
            "    Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.DateType`\n",
            "    using the optionally specified format. Specify formats according to `datetime pattern`_.\n",
            "    By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n",
            "    is omitted. Equivalent to ``col.cast(\"date\")``.\n",
            "    \n",
            "    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
            "    \n",
            "    .. versionadded:: 2.2.0\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
            "    >>> df.select(to_date(df.t).alias('date')).collect()\n",
            "    [Row(date=datetime.date(1997, 2, 28))]\n",
            "    \n",
            "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
            "    >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n",
            "    [Row(date=datetime.date(1997, 2, 28))]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy.select(f.to_date(f.lit('20220312'), 'yyyyMMdd').alias(\"date\")).printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nddP2Ty9eUAo",
        "outputId": "04a6afef-0f1d-45de-b3cc-683d70469536"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy.select(f.to_timestamp(f.lit('20220312'), 'yyyyMMdd').alias(\"date\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipR0JqmBegXX",
        "outputId": "d2809839-7c6a-429e-e1c0-e64dc58a56d9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|               date|\n",
            "+-------------------+\n",
            "|2022-03-12 00:00:00|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date Arithmetics"
      ],
      "metadata": {
        "id": "kDwv5yOPfK4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
        "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
        "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
        "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
        "                ]"
      ],
      "metadata": {
        "id": "WV5P_uVVe0mq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datetimeDf= spark.createDataFrame(datetimes, \"date string, datetime string\")"
      ],
      "metadata": {
        "id": "JzVfhH9Dfa7I"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datetimeDf.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H31d69v9fku3",
        "outputId": "da90b6be-48ce-437b-8ac4-ce8ef3b43755"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------------------+\n",
            "|date      |datetime               |\n",
            "+----------+-----------------------+\n",
            "|2014-02-28|2014-02-28 10:00:00.123|\n",
            "|2016-02-29|2016-02-29 08:08:08.999|\n",
            "|2017-10-31|2017-12-31 11:59:59.123|\n",
            "|2019-11-30|2019-08-31 00:00:00.000|\n",
            "+----------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(f.date_add)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HIzEWSqftm8",
        "outputId": "dad8a05c-2698-4785-9ed7-afe5a009a880"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function date_add in module pyspark.sql.functions:\n",
            "\n",
            "date_add(start, days)\n",
            "    Returns the date that is `days` days after `start`\n",
            "    \n",
            "    .. versionadded:: 1.5.0\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
            "    >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n",
            "    [Row(next_date=datetime.date(2015, 4, 9))]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## adding and subtracting 10 to both the dates.\n",
        "(datetimeDf\n",
        "  .withColumn(\"date_add\", f.date_add('date', 10))\n",
        "  .withColumn(\"datetimee_add\", f.date_add('datetime', 10))\n",
        "  .withColumn(\"date_sub\", f.date_sub('date', 10))\n",
        "  .withColumn('datetime_sub', f.date_sub('datetime', 10))\n",
        " ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnnvTC0FfmuY",
        "outputId": "c9c92b0d-7d6a-4785-83ac-a67f757d59d1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+----------+-------------+----------+------------+\n",
            "|      date|            datetime|  date_add|datetimee_add|  date_sub|datetime_sub|\n",
            "+----------+--------------------+----------+-------------+----------+------------+\n",
            "|2014-02-28|2014-02-28 10:00:...|2014-03-10|   2014-03-10|2014-02-18|  2014-02-18|\n",
            "|2016-02-29|2016-02-29 08:08:...|2016-03-10|   2016-03-10|2016-02-19|  2016-02-19|\n",
            "|2017-10-31|2017-12-31 11:59:...|2017-11-10|   2018-01-10|2017-10-21|  2017-12-21|\n",
            "|2019-11-30|2019-08-31 00:00:...|2019-12-10|   2019-09-10|2019-11-20|  2019-08-21|\n",
            "+----------+--------------------+----------+-------------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(f.datediff)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g2bpOJMgl9J",
        "outputId": "986d1ccc-5d36-4850-c33d-b5b994bceefe"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function datediff in module pyspark.sql.functions:\n",
            "\n",
            "datediff(end, start)\n",
            "    Returns the number of days from `start` to `end`.\n",
            "    \n",
            "    .. versionadded:: 1.5.0\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
            "    >>> df.select(datediff(df.d2, df.d1).alias('diff')).collect()\n",
            "    [Row(diff=32)]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the numbers of months between current_date and date values.\n",
        "(\n",
        "    datetimeDf\n",
        "      .withColumn('monthsBetween', f.months_between(f.current_date(), 'date'))\n",
        "      .withColumn('addMonths', f.add_months('date', 3))\n",
        ").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcb-DbaMg4iX",
        "outputId": "94958551-149f-477c-99ad-f4925188419a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-------------+----------+\n",
            "|      date|            datetime|monthsBetween| addMonths|\n",
            "+----------+--------------------+-------------+----------+\n",
            "|2014-02-28|2014-02-28 10:00:...|   98.5483871|2014-05-28|\n",
            "|2016-02-29|2016-02-29 08:08:...|  74.51612903|2016-05-29|\n",
            "|2017-10-31|2017-12-31 11:59:...|   54.4516129|2018-01-31|\n",
            "|2019-11-30|2019-08-31 00:00:...|  29.48387097|2020-02-29|\n",
            "+----------+--------------------+-------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(f.trunc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ief48k5HnCXK",
        "outputId": "30e621f3-9c2b-41a0-a2e7-895e13ea5a35"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function trunc in module pyspark.sql.functions:\n",
            "\n",
            "trunc(date, format)\n",
            "    Returns date truncated to the unit specified by the format.\n",
            "    \n",
            "    .. versionadded:: 1.5.0\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    date : :class:`~pyspark.sql.Column` or str\n",
            "    format : str\n",
            "        'year', 'yyyy', 'yy' to truncate by year,\n",
            "        or 'month', 'mon', 'mm' to truncate by month\n",
            "        Other options are: 'week', 'quarter'\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df = spark.createDataFrame([('1997-02-28',)], ['d'])\n",
            "    >>> df.select(trunc(df.d, 'year').alias('year')).collect()\n",
            "    [Row(year=datetime.date(1997, 1, 1))]\n",
            "    >>> df.select(trunc(df.d, 'mon').alias('month')).collect()\n",
            "    [Row(month=datetime.date(1997, 2, 1))]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(\n",
        "    datetimeDf\n",
        "    .withColumn(\"trunc_date\", f.trunc('date', \"mm\"))\n",
        "    .withColumn('trunc_tme', f.trunc('datetime', 'yy'))\n",
        "    \n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPBI0eZenoG5",
        "outputId": "bc468892-399d-4dec-c82b-20b47909aa9a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+----------+----------+\n",
            "|      date|            datetime|trunc_date| trunc_tme|\n",
            "+----------+--------------------+----------+----------+\n",
            "|2014-02-28|2014-02-28 10:00:...|2014-02-01|2014-01-01|\n",
            "|2016-02-29|2016-02-29 08:08:...|2016-02-01|2016-01-01|\n",
            "|2017-10-31|2017-12-31 11:59:...|2017-10-01|2017-01-01|\n",
            "|2019-11-30|2019-08-31 00:00:...|2019-11-01|2019-01-01|\n",
            "+----------+--------------------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(f.date_trunc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbaAB9CUn-8J",
        "outputId": "116fa4d6-a662-4ab5-bc0e-a1fd4d4da5a4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function date_trunc in module pyspark.sql.functions:\n",
            "\n",
            "date_trunc(format, timestamp)\n",
            "    Returns timestamp truncated to the unit specified by the format.\n",
            "    \n",
            "    .. versionadded:: 2.3.0\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    format : str\n",
            "        'year', 'yyyy', 'yy' to truncate by year,\n",
            "        'month', 'mon', 'mm' to truncate by month,\n",
            "        'day', 'dd' to truncate by day,\n",
            "        Other options are:\n",
            "        'microsecond', 'millisecond', 'second', 'minute', 'hour', 'week', 'quarter'\n",
            "    timestamp : :class:`~pyspark.sql.Column` or str\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n",
            "    >>> df.select(date_trunc('year', df.t).alias('year')).collect()\n",
            "    [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\n",
            "    >>> df.select(date_trunc('mon', df.t).alias('month')).collect()\n",
            "    [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(\n",
        "    datetimeDf\n",
        "    .withColumn('date_truc', f.date_trunc('MM', 'date'))\n",
        "    .withColumn('time_truc', f.date_trunc('MM', 'datetime'))\n",
        "    .withColumn('time_truc2', f.date_trunc('Hour', 'datetime'))\n",
        "\n",
        "\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "durEB5GOoG0q",
        "outputId": "0b8e0bd5-3c58-42c2-a056-c34851670326"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-------------------+-------------------+-------------------+\n",
            "|      date|            datetime|          date_truc|          time_truc|         time_truc2|\n",
            "+----------+--------------------+-------------------+-------------------+-------------------+\n",
            "|2014-02-28|2014-02-28 10:00:...|2014-02-01 00:00:00|2014-02-01 00:00:00|2014-02-28 10:00:00|\n",
            "|2016-02-29|2016-02-29 08:08:...|2016-02-01 00:00:00|2016-02-01 00:00:00|2016-02-29 08:00:00|\n",
            "|2017-10-31|2017-12-31 11:59:...|2017-10-01 00:00:00|2017-12-01 00:00:00|2017-12-31 11:00:00|\n",
            "|2019-11-30|2019-08-31 00:00:...|2019-11-01 00:00:00|2019-08-01 00:00:00|2019-08-31 00:00:00|\n",
            "+----------+--------------------+-------------------+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(f.date_format)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGTdmBLaoTe6",
        "outputId": "83fe248d-5d55-43e3-a334-536783a6e19f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function date_format in module pyspark.sql.functions:\n",
            "\n",
            "date_format(date, format)\n",
            "    Converts a date/timestamp/string to a value of string in the format specified by the date\n",
            "    format given by the second argument.\n",
            "    \n",
            "    A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n",
            "    pattern letters of `datetime pattern`_. can be used.\n",
            "    \n",
            "    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
            "    \n",
            "    .. versionadded:: 1.5.0\n",
            "    \n",
            "    Notes\n",
            "    -----\n",
            "    Whenever possible, use specialized functions like `year`.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
            "    >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n",
            "    [Row(date='04/08/2015')]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CK82mOGrzh-M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}